---
title: 'LangChain & RAG Setup'
description: 'Configure LangChain for AI-Powered Semantic Search and RAG Workflows'
icon: 'sparkles'
---

## Overview

LangChain orchestrates AI workflows between Ollama (LLM), Qdrant (Vector DB), and your Next.js application. This enables Retrieval-Augmented Generation (RAG) for intelligent document search and analysis.

## Container 3: Ollama LLM

**Technology:** Ollama + LangChain  
**Purpose:** Local LLM inference and embedding generation  
**Port:** 11434  
**Models:** Llama 3, Mistral, CodeLlama

### Available Models

```bash
# List available models
curl http://localhost:11434/api/tags

# Pull new model
curl http://localhost:11434/api/pull -d '{"name": "mistral"}'
```

### LLM Inference

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "mistral",
  "prompt": "Why is the sky blue?",
  "stream": false
}'
```

## Container 4: Qdrant Vector Database

**Technology:** Qdrant  
**Purpose:** High-performance vector similarity search  
**Port:** 6333  
**Use Cases:** Semantic search, RAG document retrieval

### Initialize Collection

```bash
curl -X PUT http://qdrant:6333/collections/documents \
  -H "Content-Type: application/json" \
  -d '{
    "vectors": {
      "size": 1536,
      "distance": "Cosine"
    }
  }'
```

## LangChain Integration

### Setup

```typescript lib/langchain/setup.ts
import { ChatOllama } from '@langchain/community/chat_models/ollama';
import { OllamaEmbeddings } from '@langchain/community/embeddings/ollama';
import { QdrantVectorStore } from '@langchain/community/vectorstores/qdrant';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';

// Initialize components
export const llm = new ChatOllama({
  model: 'mistral',
  baseUrl: 'http://ollama:11434',
  temperature: 0.7
});

export const embeddings = new OllamaEmbeddings({
  model: 'mistral',
  baseUrl: 'http://ollama:11434'
});

export const vectorStore = new QdrantVectorStore(embeddings, {
  url: 'http://qdrant:6333',
  collectionName: 'documents',
  // Qdrant API key if needed
  apiKey: process.env.QDRANT_API_KEY
});
```

### Document Ingestion

```typescript lib/langchain/ingest.ts
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';
import { vectorStore } from './setup';

export async function ingestDocument(content: string, metadata: Record<string, any>) {
  // Split document into chunks
  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200
  });

  const chunks = await splitter.splitText(content);

  // Add to vector store
  await vectorStore.addDocuments(
    chunks.map((text, i) => ({
      pageContent: text,
      metadata: { ...metadata, chunk: i }
    }))
  );
}
```

### RAG Query

```typescript lib/langchain/rag.ts
import { createRetrievalChain } from 'langchain/chains/retrieval';
import { createStuffDocumentsChain } from 'langchain/chains/stuff_documents_chain';
import { llm, vectorStore } from './setup';

export async function ragQuery(userQuery: string) {
  const retriever = vectorStore.asRetriever({
    k: 5  // Return top 5 most similar documents
  });

  const chain = createRetrievalChain({
    retriever,
    combineDocumentsChain: createStuffDocumentsChain({
      llm
    })
  });

  const result = await chain.invoke({
    input: userQuery
  });

  return result.output;
}
```

## Integration with Next.js

### API Route

```typescript pages/api/ai/search.ts
import { ragQuery } from '@/lib/langchain/rag';

export default async function handler(req, res) {
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const { query } = req.body;
    const result = await ragQuery(query);
    res.status(200).json({ result });
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: 'Search failed' });
  }
}
```

### Frontend Component

```typescript components/AISearch.tsx
import { useState } from 'react';

export function AISearch() {
  const [query, setQuery] = useState('');
  const [result, setResult] = useState('');
  const [loading, setLoading] = useState(false);

  const handleSearch = async (e: React.FormEvent) => {
    e.preventDefault();
    setLoading(true);
    try {
      const res = await fetch('/api/ai/search', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ query })
      });
      const data = await res.json();
      setResult(data.result);
    } finally {
      setLoading(false);
    }
  };

  return (
    <form onSubmit={handleSearch}>
      <input
        value={query}
        onChange={(e) => setQuery(e.target.value)}
        placeholder="Ask a question..."
      />
      <button type="submit" disabled={loading}>
        {loading ? 'Searching...' : 'Search'}
      </button>
      {result && <div>{result}</div>}
    </form>
  );
}
```

## SQL Snapshot Integration

### Document Pipeline

```typescript lib/langchain/snapshot.ts
import { ingestDocument } from './ingest';
import prisma from '@/lib/prisma';

export async function processSnapshot() {
  // Get data from PostgreSQL
  const customers = await prisma.customer.findMany({
    include: { orders: true }
  });

  // Prepare content
  const content = customers
    .map(c => `Customer: ${c.name}\nOrders: ${c.orders.length}`)
    .join('\n---\n');

  // Ingest into RAG
  await ingestDocument(content, {
    source: 'sql_snapshot',
    timestamp: new Date()
  });
}
```

## Monitoring & Troubleshooting

<AccordionGroup>
  <Accordion title="Ollama not responding">
    ```bash
    # Check if Ollama is running
    docker-compose ps ollama
    
    # View logs
    docker-compose logs ollama
    
    # Test connection
    curl http://localhost:11434/api/tags
    ```
  </Accordion>

  <Accordion title="Vector search slow">
    - Reduce chunk size (improves precision)
    - Add index to Qdrant collection
    - Consider batch ingestion for large datasets
  </Accordion>

  <Accordion title="Memory issues">
    - Ollama can be memory-intensive
    - Reduce model size (use `mistral` instead of `llama2`)
    - Increase container memory limit in docker-compose.yml
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>

<Card title="Database Setup" icon="database" href="/essentials/navigation">
  Configure PostgreSQL and Neo4j
</Card>

<Card title="API Integration" icon="code" href="/api-reference/introduction">
  Expose RAG via REST API
</Card>

<Card title="Code Examples" icon="terminal" href="/essentials/code">
  View implementation patterns
</Card>

</CardGroup>
